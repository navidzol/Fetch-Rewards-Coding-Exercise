# Packages require for this module
import numpy as np   # for mathematical evaluations
import re   # for string manipulations

class TextComparison:
    def __init__(self, text1, text2):
        """
        Giving two texts to compare.
        :param text1: is string of any length.
        :param text2: is string of any length.
        """
        self.text1 = text1
        self.text2 = text2
        self.punctuations = '!"#$%&()*+, -./:;<=>?@[\]^_`{|}~' + "'"  # We could also used the string library
        # (string.punctuation) to get these punctuations

    def _text_splitter(self):
        """
        This method will look into the texts that are given and create a database of the words inside it using
        dictionaries. I provide two metric, one with considering punctuation and one ignoring them. Therefore in this
        method we produce two types of data structures for each texts, one considering punctuations and one without it.
        :return:
        a data structure that contains four dictionaries.
        """
        # first preparing the texts by making them lower case and expanding the cases where the words that summarized
        # like changing "we'll" to "we will" or "he isn't" to "he is not":
        str1 = self.text1.lower().replace("'ll", ' will').replace("n't", ' not')
        str2 = self.text2.lower().replace("'ll", ' will').replace("n't", ' not')

        # now making a dictionary of the unique words in each string:
        key1 = set([t.strip(self.punctuations) for t in str1.split() if ((len(t.strip(self.punctuations)) != 0) &
                                                                         (t.strip(self.punctuations) != " "))])
        key2 = set([t.strip(self.punctuations) for t in str2.split() if ((len(t.strip(self.punctuations)) != 0) &
                                                                         (t.strip(self.punctuations) != " "))])
        keys = key1.union(key2)  # finding the union of two keys group from each texts

        # This is a version that keep the punctuations as a separate word:
        key1_with_punc = set([t for t in re.split('(\W)', str1) if ((len(t) != 0) & (t != " "))])
        key2_with_punc = set([t for t in re.split('(\W)', str2) if ((len(t) != 0) & (t != " "))])
        keys_with_punc = key1_with_punc.union(key2_with_punc)  # finding the union of two keys group from each texts

        data = {}
        # creating the dictionaries:
        data['text1_dict_without_punc'] = {key: str1.count(key) for key in keys}
        data['text2_dict_without_punc'] = {key: str2.count(key) for key in keys}
        data['text1_dict_with_punc'] = {key: str1.count(key) for key in keys_with_punc}
        data['text2_dict_with_punc'] = {key: str2.count(key) for key in keys_with_punc}
        return data

    def score_evaluation(self):
        """
        Providing a unique numbers as the metric of the evaluation. If the inputs are only a word then the output is the
        hamming distant of two words normalized by the length of the biggest word. Otherwise I give the cosine
        similarity as the metric of comparison.
        :return: I give two numbers as output, the first one is when you ignore the punctuation and the second one is
                when you consider that.
        """
        # parsing texts into a data structure:
        data = self._text_splitter()

        if (len(keys_without_punc) == 2) and (np.min(list(data['text1_dict_without_punc'].values())) == 0) \
                and (np.min(list(data['text2_dict_without_punc'].values())) == 0):
            # In this case we are only comparing a words so I used Hamming distance as a measure:

            # Getting the two words:
            keys_without_punc = data['text1_dict_without_punc'].keys()

            # converting the dict keys to list:
            list_keys = list(keys_without_punc)

            # Finding the length of the largest word without considering the punctuations:
            maximum_key_length = max([len(key) for key in keys_without_punc])

            # Aggregate the words into a tuple for finding the character by character comparison:
            zipped_words = zip(list_keys[0], list_keys[1])

            # Finding the Hamming distance:
            hamming_distance = sum(xi == yi for xi, yi in zipped_words)

            # Returning the normalized Hamming distance:
            return hamming_distance / maximum_key_length
        else:
            # Vectoring the texts:
            vectorize_without_punc = _vectoring_dicts(data['text1_dict_without_punc'], data['text2_dict_without_punc'])
            vectorize_with_punc = _vectoring_dicts(data['text1_dict_with_punc'], data['text2_dict_with_punc'])

            # Finding the cosine similarity metrics:
            cosine_similiarity_without_punc, _ = _cosine_similiarity_calculator(
                vectorize_without_punc[0], vectorize_without_punc[1])
            cosine_similiarity_wit_punc, _ = _cosine_similiarity_calculator( vectorize_with_punc[0],
                                                                                            vectorize_with_punc[1])

            return cosine_similiarity_without_punc, cosine_similiarity_wit_punc

def _vectoring_dicts(dict1, dict2):
    # creating a vectorized format of the texts:
    vec_1 = np.array(list(dict1.values()))
    vec_2 = np.array(list(dict2.values()))
    return vec_1, vec_2

def _cosine_similiarity_calculator(vec_1, vec_2):
    """
    The inputs are the numpy array with length n (same length for both vectors) and the output will be the cosine
    of the angle between these two vectors:
                        Cos( theta ) := (vec_1 . vec_2) / (norm(vec_1) x norm(vec_2))
    :param vec_1: (numpy array) first vector
    :param vec_2: (numpy array) second vector
    :return: (a list of two floats numbers) the first item of the list is a number that represent the cosine of the
            angle between vec_1 and vec_2. This is a float number that can has a range of [-1, 1]. The second item
            is also a float number but is (1 + cos( theta )) / 2. This is a number like cosine similarity but is in
            the range of [0, 1].
    """
    # making sure that the given vectors are 1d numpy arrays:
    if isinstance(vec_1, list):
        vec_1 = np.array(vec_1, ndmin=1)
    elif isinstance(vec_1, np.ndarray) and (len(vec_1.shape) > 1):
        vec_1 = vec_1.flatten()

    if isinstance(vec_2, list):
        vec_2 = np.array(vec_2, ndmin=1)
    elif isinstance(vec_2, np.ndarray) and (len(vec_2.shape) > 1):
        vec_2 = vec_2.flatten()

    # Finding the unit vector of each given vectors:
    unit_vec_1 = vec_1 / np.linalg.norm(vec_1)

    unit_vec_2 = vec_2 / np.linalg.norm(vec_2)

    # Finding the dot products:
    dot_product = np.dot(unit_vec_1, unit_vec_2)

    # Returning the dot products and the scaled version of it:
    return [dot_product, (1 + dot_product) / 2]


